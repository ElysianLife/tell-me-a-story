{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Tell Me a Story...#\n",
    "\n",
    "Elyse Renouf   **|**   July 31, 2020   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell Me A Story (TMAS) is a content-based young children's book recommender using NLP that is derived from a Kaggle dataset of Goodreads.com book reviews. It was created so parents could input one of their child's favourite books and TMAS can recommend other similar books to read based entirely on key words in the book descriptions. \n",
    "\n",
    "**Please Note:** This is notebook 3 of 3 that were used to build the final product. This notebook includes the vectorization steps I took and the final recommender model that is TMAS. \n",
    "\n",
    "<hr>\n",
    "\n",
    "### Data Vectorization & Building the Recommender Model ###\n",
    "\n",
    "Because this is a content-based recommender, the key preparation here is ensuring all columns used in the recommender are in numerical format. So the emphasis was on text preparation. \n",
    "\n",
    "I built my tokenizer function to remove punctuation, split the sentence into individual words, stem the verbs to their root word, to remove English stop words, and to remove words shorter than 2 characters in an attempt to remove any lingering html tags from the original web scraping without removing words related to animal sounds which are key features in infant books (hee haw, baa baa, etc.). I then tested it on a simple sentence to ensure it worked as required before applying it within my model on the entire dataset. \n",
    "\n",
    "I used a helper function that would look up a books TFIDF score by the book name, then instantiated and fit the model and transformed the data. Once all the book description text was transformed, I used a function to calculate cosine similarities to determine just HOW similar one book description is to that of other books in the dataset. \n",
    "\n",
    "Once all of these steps were complete, I was able to build a Streamlit app to test version one and also to build out version two in this notebook, which allows the user to filter books returned to them by the age group column I feature engineered in earlier steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To calc cosine distance later\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the cleaned kidsbooks dataset\n",
    "books = pd.read_csv('data/books.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>is_preschooler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0688175740</td>\n",
       "      <td>russell's secret</td>\n",
       "      <td>have you ever heard the words&lt;br /&gt;\"\"you can s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1582460892</td>\n",
       "      <td>oye, hormiguita</td>\n",
       "      <td>the spanish translation of bestseller hey, lit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1904442978</td>\n",
       "      <td>dear bunny</td>\n",
       "      <td>the cutest couple of star-crossed rabbits craf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0689840047</td>\n",
       "      <td>a revolutionary field trip: poems of colonial ...</td>\n",
       "      <td>the class is embarking on the field trip of a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1905236816</td>\n",
       "      <td>home for a tiger, home for a bear</td>\n",
       "      <td>you'll learn about the habitats of these and m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0439598397</td>\n",
       "      <td>dog breath! the horrible trouble with hally to...</td>\n",
       "      <td>hally tosis is a very good dog, but she has a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0312367511</td>\n",
       "      <td>action jackson</td>\n",
       "      <td>one late spring morning the american artist ja...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0590898264</td>\n",
       "      <td>cinderella bigfoot</td>\n",
       "      <td>head for the hills! the author and illustrator...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0618064877</td>\n",
       "      <td>nursery crimes</td>\n",
       "      <td>jambo and marva emigrated from france to iowa ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0887764746</td>\n",
       "      <td>the legend of the panda</td>\n",
       "      <td>&lt;i&gt;a timeless tale about a beloved animal&lt;/i&gt;&lt;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         isbn                                               name  \\\n",
       "0  0688175740                                   russell's secret   \n",
       "1  1582460892                                    oye, hormiguita   \n",
       "2  1904442978                                         dear bunny   \n",
       "3  0689840047  a revolutionary field trip: poems of colonial ...   \n",
       "4  1905236816                  home for a tiger, home for a bear   \n",
       "5  0439598397  dog breath! the horrible trouble with hally to...   \n",
       "6  0312367511                                     action jackson   \n",
       "7  0590898264                                 cinderella bigfoot   \n",
       "8  0618064877                                     nursery crimes   \n",
       "9  0887764746                            the legend of the panda   \n",
       "\n",
       "                                         description  is_preschooler  \n",
       "0  have you ever heard the words<br />\"\"you can s...               1  \n",
       "1  the spanish translation of bestseller hey, lit...               1  \n",
       "2  the cutest couple of star-crossed rabbits craf...               1  \n",
       "3  the class is embarking on the field trip of a ...               1  \n",
       "4  you'll learn about the habitats of these and m...               1  \n",
       "5  hally tosis is a very good dog, but she has a ...               1  \n",
       "6  one late spring morning the american artist ja...               1  \n",
       "7  head for the hills! the author and illustrator...               1  \n",
       "8  jambo and marva emigrated from france to iowa ...               1  \n",
       "9  <i>a timeless tale about a beloved animal</i><...               0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_descriptions = books[['isbn', 'name', 'description', 'is_preschooler']]\n",
    "df_descriptions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2961, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating separate dfs for all infant books\n",
    "infbooks = df_descriptions[df_descriptions['is_preschooler']==0]\n",
    "\n",
    "#and exporting as CSV for safekeeping\n",
    "infbooks.to_csv('infbooks.csv', header=True)\n",
    "infbooks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7429, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating separate dfs for all preschooler books\n",
    "preschoolbooks = df_descriptions[df_descriptions['is_preschooler']==1]\n",
    "\n",
    "#and exporting as CSV for safekeeping\n",
    "preschoolbooks.to_csv('preschoolbooks.csv', header=True)\n",
    "preschoolbooks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/elyserenouf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#create tokenizer function to remove punctuation, split sentence, stem words, and remove stopwords\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#remove english stop words like the, and, if, a, etc. \n",
    "ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def my_tokenizer(sentence):\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        # Remove punctuation and re-set to lower case\n",
    "        sentence = sentence.replace(punctuation_mark,'').lower() #includes !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~             \n",
    "    # split sentence into words\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []     \n",
    "    # Remove stopwords and any tokens that are just empty strings\n",
    "    for word in listofwords:\n",
    "        if (not word in ENGLISH_STOP_WORDS) and (word!='') and (len(word)>2):\n",
    "            # Stem words\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            listofstemmed_words.append(stemmed_word)\n",
    "    return listofstemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sentence used for checking our process\n",
    "sentence = \"I was walking down the road and I saw a donkey <br> Hee Haw <br>!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walk', 'road', 'saw', 'donkey', 'hee', 'haw']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking tokenizer on simple sentence from above\n",
    "my_tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elyserenouf/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/elyserenouf/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'thu', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<10390x6451 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 285358 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doing some word frequency discovery: using my tokenizer and just simply countvectorizing words\n",
    "# because this isn't to determine/predict positive/negative reviews but simply to get word counts, I will do this on the entire dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#instantiate the model\n",
    "bagofwords = CountVectorizer(stop_words=\"english\", min_df=5, tokenizer=my_tokenizer)\n",
    "\n",
    "#fit the model\n",
    "bagofwords.fit(df_descriptions['description'].fillna(''))\n",
    "\n",
    "#transform the data\n",
    "allwords = bagofwords.transform(df_descriptions['description'].fillna(''))\n",
    "allwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>book</td>\n",
       "      <td>4755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5480</th>\n",
       "      <td>stori</td>\n",
       "      <td>2683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>children</td>\n",
       "      <td>2676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>illustr</td>\n",
       "      <td>2649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>littl</td>\n",
       "      <td>2534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>make</td>\n",
       "      <td>1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3859</th>\n",
       "      <td>new</td>\n",
       "      <td>1876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6416</th>\n",
       "      <td>young</td>\n",
       "      <td>1859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>day</td>\n",
       "      <td>1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>reader</td>\n",
       "      <td>1783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>love</td>\n",
       "      <td>1650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>color</td>\n",
       "      <td>1555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5816</th>\n",
       "      <td>time</td>\n",
       "      <td>1421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>friend</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>come</td>\n",
       "      <td>1339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>help</td>\n",
       "      <td>1336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>anim</td>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>learn</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4572</th>\n",
       "      <td>read</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>pictur</td>\n",
       "      <td>1285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6221</th>\n",
       "      <td>way</td>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362</th>\n",
       "      <td>world</td>\n",
       "      <td>1227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3353</th>\n",
       "      <td>like</td>\n",
       "      <td>1189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>fun</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>live</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "676       book   4755\n",
       "5480     stori   2683\n",
       "1021  children   2676\n",
       "2868   illustr   2649\n",
       "3379     littl   2534\n",
       "3496      make   1964\n",
       "3859       new   1876\n",
       "6416     young   1859\n",
       "1448       day   1852\n",
       "4578    reader   1783\n",
       "3436      love   1650\n",
       "1157     color   1555\n",
       "5816      time   1421\n",
       "2275    friend   1397\n",
       "1166      come   1339\n",
       "2651      help   1336\n",
       "248       anim   1333\n",
       "3297     learn   1307\n",
       "4572      read   1296\n",
       "4223    pictur   1285\n",
       "6221       way   1248\n",
       "6362     world   1227\n",
       "3353      like   1189\n",
       "2303       fun   1177\n",
       "3381      live   1100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's put the allwords sparse matrix information into a data frame\n",
    "word_counts = np.array(np.sum(allwords, axis=0)).reshape((-1,))\n",
    "words = np.array(bagofwords.get_feature_names())\n",
    "words_df = pd.DataFrame({\"word\":words, \n",
    "                         \"count\":word_counts})\n",
    "\n",
    "#finding 25 most frequently used words in these book descriptions\n",
    "words_df.sort_values(by=\"count\", ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if I were to try using CountVectorizer for my recommender function, all the words in the description end up being equally important but would factor more heavily the frequency of words in order to make book description matches - this doesn't make sense for a recommender machine, especially because the words that are most frequently used in children's book descriptions are pretty generic. \n",
    "\n",
    "For future iterations and increased performance, I would investigate more closely and potentially break down the words by part of speech (noun, verb, adjective), and create a list of the frequently appearing words to drop that don't really easily describe the story itself (book, reader, story, like, illustration, know, use, tale, etc.).\n",
    "\n",
    "As it stands, I have decided to use the TFIDF vectorizer which will put more emphasis/weight on less frequently appearing words, which are likely more descriptive/indicative of what a book is actually about. Many book descriptions include the same words (see above) but the key nouns, adjectives, and verbs specific to just that book are the ones that really tell us what the story is about and will be the ones that allow us to find similar stories using those specific nouns and adjectives:\n",
    "\n",
    "eg. \"This story is about a boy and a giraffe who fly to the moon\"  - will likely have more emphasis on giraffe, moon, and fly in the overall dataset, remove the stop words, and less emphasis on story or boy because of their likelihood of appearing in multiple stories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And Now To Vectorize, Apply My Tokenizer, and Build The Recommender: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need this helper function to look up a book TFIDF by its name.\n",
    "def get_book_by_name(name, tfidf_scores, keys):\n",
    "    row_id = keys[name]\n",
    "    row = tfidf_scores[row_id,:]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elyserenouf/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "/Users/elyserenouf/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'thu', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10390, 6451)\n",
      "(10390, 3)\n"
     ]
    }
   ],
   "source": [
    "#importing TFIDF vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Instantiating the TFIDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = \"english\", min_df=5, tokenizer=my_tokenizer)\n",
    "\n",
    "index = 0\n",
    "keys = {}\n",
    "\n",
    "df_descriptions = books[['name','description', 'is_preschooler']]\n",
    "\n",
    "#set a loop for book results lookup and return\n",
    "for book in df_descriptions.itertuples() :\n",
    "    key = book[1]\n",
    "    keys[key] = index\n",
    "    index += 1\n",
    "\n",
    "#Fit the vectorizer to the data\n",
    "vectorizer.fit(df_descriptions['description'].fillna(''))\n",
    "\n",
    "#Transform the data\n",
    "tfidf_scores = vectorizer.transform(df_descriptions['description'].fillna(''))\n",
    "\n",
    "print(tfidf_scores.shape)\n",
    "print(df_descriptions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to calculate how similar one book description is to the other book description\n",
    "\n",
    "#import cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#create content recommender function that also takes into account age group\n",
    "def content_recommender(name, is_preschooler, tfidf_scores, bookdf=df_descriptions) :\n",
    "    \n",
    "    #if statement to filter books by is_preschooler column\n",
    "    if is_preschooler==True:\n",
    "        bookdf = bookdf[bookdf['is_preschooler']== 1]\n",
    "    else: \n",
    "        bookdf = bookdf[bookdf['is_preschooler']== 0]\n",
    "        \n",
    "    #Store the results in this DF\n",
    "    similar_books = pd.DataFrame(columns = [\"name\",\"similarity\"] )\n",
    "    \n",
    "    #The book we are finding books similar to\n",
    "    book_1 = get_book_by_name(name, tfidf_scores, keys)\n",
    "    \n",
    "    #Go through ALL the books\n",
    "    for book in bookdf['name']:\n",
    "                \n",
    "        #Find the similarity of the two books\n",
    "        book_2 = get_book_by_name(book,tfidf_scores,keys)\n",
    "        similarity = cosine_similarity(book_1,book_2)\n",
    "        similar_books.loc[len(similar_books)] = [book, similarity[0][0]]\n",
    "\n",
    "    return similar_books.sort_values(by=['similarity'],ascending=False)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'the lion inside'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1cdca3bef902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#the moment of truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#type in a book (known to be in the database), select True for books for kids 3+ years old (30 pages+) otherwise false\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msimilar_books\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_recommender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the lion inside\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#return a list of 10 similar books in descending similarity order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msimilar_books\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-4dea0a523900>\u001b[0m in \u001b[0;36mcontent_recommender\u001b[0;34m(name, is_preschooler, tfidf_scores, bookdf)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#The book we are finding books similar to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mbook_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_book_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#Go through ALL the books\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7e73df1f4ca0>\u001b[0m in \u001b[0;36mget_book_by_name\u001b[0;34m(name, tfidf_scores, keys)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Need this helper function to look up a book TFIDF by its name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_book_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrow_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'the lion inside'"
     ]
    }
   ],
   "source": [
    "#the moment of truth\n",
    "#type in a book (known to be in the database), select True for books for kids 3+ years old (30 pages+) otherwise false\n",
    "similar_books = content_recommender(\"the lion inside\", True, tfidf_scores)\n",
    "#return a list of 10 similar books in descending similarity order\n",
    "similar_books.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So...what? Is this a good score?### \n",
    "\n",
    "In order to determine whether the scores my recommender function have given are actually \"good\", I decided to do a test based on my knowledge of some of the books that exist in the dataframe.\n",
    "\n",
    "I was able to see that when two very different books were put in the recommender (very different in description, subject matter, and key words), the similarity score was ~4%. And when I entered in two very similar books (two books from the Clifford the Big Red Dog series), the similarity score was ~24%. \n",
    "\n",
    "So, for the purpose of this analysis, using those similarity scores as a benchmark, it looks like my recommender is working very well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a test function to compare cosine similarities\n",
    "get_book_by_name(\"the velveteen rabbit\", tfidf_scores, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing out similarities between two very different kids books \n",
    "book_1 = get_book_by_name('the velveteen rabbit', tfidf_scores, keys)\n",
    "book_2 = get_book_by_name('a big city abc', tfidf_scores, keys)\n",
    "\n",
    "print(\"Similarity:\", cosine_similarity(book_1, book_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing out similarities between two very similar kids books \n",
    "book_1 = get_book_by_name('a charlie brown christmas', tfidf_scores, keys)\n",
    "book_2 = get_book_by_name('a charlie brown valentine', tfidf_scores, keys)\n",
    "\n",
    "print(\"Similarity:\", cosine_similarity(book_1, book_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So once I had a working recommender, in order to make it more interactive for presentation and demonstration purposes, I built a streamlit app. \n",
    "\n",
    "The cells below are just some snippets of code that helped me to form my final streamlit app code which is included in my project folder and is named TMAS-app.py. \n",
    "\n",
    "My initial app prototype (as it works now) is a simple recommender, not filtered by age. It includes a dropdown selection box where users can select what book they are reading with their kids now and then it will recommend a few titles they could try out based on similarity of some of the words in their descriptions. \n",
    "\n",
    "There is also an option to click on a selectbox at the bottom of the app page and see the entire book list dataframe for futher exploration. Unfortunately it is not live so I have included screenshots of this in my presentation slide deck and demo video. \n",
    "\n",
    "Future iterations (prototype can be found in file TMAS-app-filtered.py) would create a checkbox that allows the user to filter that initial dropdown list by the age of their child...so they would select 0-2 and would only see books categorized as is_preschooler=0 in the selection list, they will also only be recommended similar books from the full infant/toddler book list. \n",
    "\n",
    "For additional complexity and more reliable/realistic results, it would be better to get access to the GoodReads.com API and source books with an existing age and book type category which would allow the user to filter and be matched with books even more relevant/similar to the books they are currently reading with their children. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing a Few Things For The Streamlit App: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6492               \"a\" is for airplane / \"a\" es para avion\n",
       "8778                     \"a\" is for animals: an abc pop-up\n",
       "5655                                   \"bee my valentine!\"\n",
       "3933                          \"let's get a pup!\" said kate\n",
       "7828                          \"let's get a pup!\" said kate\n",
       "                               ...                        \n",
       "5283                                   zuzu's wishing cake\n",
       "15624                                 ¡me gusta mi sombra!\n",
       "1344                                 ¡no me gusta mi mono!\n",
       "1147     ¡sí, se puede! / yes, we can!: janitor strike ...\n",
       "17568                                        ¿quién salta?\n",
       "Name: name, Length: 10390, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for streamlit app for sorting the name list\n",
    "df_descriptions['name'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>is_preschooler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0688175740</td>\n",
       "      <td>russell's secret</td>\n",
       "      <td>have you ever heard the words&lt;br /&gt;\"\"you can s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1582460892</td>\n",
       "      <td>oye, hormiguita</td>\n",
       "      <td>the spanish translation of bestseller hey, lit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1904442978</td>\n",
       "      <td>dear bunny</td>\n",
       "      <td>the cutest couple of star-crossed rabbits craf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0689840047</td>\n",
       "      <td>a revolutionary field trip: poems of colonial ...</td>\n",
       "      <td>the class is embarking on the field trip of a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1905236816</td>\n",
       "      <td>home for a tiger, home for a bear</td>\n",
       "      <td>you'll learn about the habitats of these and m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17957</th>\n",
       "      <td>0679885390</td>\n",
       "      <td>lady bug's ball</td>\n",
       "      <td>join lady bug and her gauzy-winged guests as t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17959</th>\n",
       "      <td>1581058101</td>\n",
       "      <td>uno, dos, tres: dime quien eres! (one, two, th...</td>\n",
       "      <td>children play \"one, two, three. who can it be ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17965</th>\n",
       "      <td>0736424954</td>\n",
       "      <td>actual fairy size (disney fairies)</td>\n",
       "      <td>the never fairies stand only five inches tall....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17974</th>\n",
       "      <td>0439545641</td>\n",
       "      <td>las tres preguntas</td>\n",
       "      <td>nikolai is a boy who believes that if he can f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17985</th>\n",
       "      <td>0679966730</td>\n",
       "      <td>a net to catch time</td>\n",
       "      <td>from the gullah culture of georgia's sea islan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7429 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             isbn                                               name  \\\n",
       "0      0688175740                                   russell's secret   \n",
       "1      1582460892                                    oye, hormiguita   \n",
       "2      1904442978                                         dear bunny   \n",
       "3      0689840047  a revolutionary field trip: poems of colonial ...   \n",
       "4      1905236816                  home for a tiger, home for a bear   \n",
       "...           ...                                                ...   \n",
       "17957  0679885390                                    lady bug's ball   \n",
       "17959  1581058101  uno, dos, tres: dime quien eres! (one, two, th...   \n",
       "17965  0736424954                 actual fairy size (disney fairies)   \n",
       "17974  0439545641                                 las tres preguntas   \n",
       "17985  0679966730                                a net to catch time   \n",
       "\n",
       "                                             description  is_preschooler  \n",
       "0      have you ever heard the words<br />\"\"you can s...               1  \n",
       "1      the spanish translation of bestseller hey, lit...               1  \n",
       "2      the cutest couple of star-crossed rabbits craf...               1  \n",
       "3      the class is embarking on the field trip of a ...               1  \n",
       "4      you'll learn about the habitats of these and m...               1  \n",
       "...                                                  ...             ...  \n",
       "17957  join lady bug and her gauzy-winged guests as t...               1  \n",
       "17959  children play \"one, two, three. who can it be ...               1  \n",
       "17965  the never fairies stand only five inches tall....               1  \n",
       "17974  nikolai is a boy who believes that if he can f...               1  \n",
       "17985  from the gullah culture of georgia's sea islan...               1  \n",
       "\n",
       "[7429 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for streamlit app for identifying the book name filter\n",
    "books[books['is_preschooler']== 1]\n",
    "\n",
    "#note: the html tag in the 0th row is present BEFORE applying the tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is TMAS as it stands today. \n",
    "\n",
    "Please see the final report and complete zip file for all project files and documentation and don't hestiate to contact me if you have any questions about this project. Thank you so much for your time and attention.\n",
    "\n",
    "**Happy reading!!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
